{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6cb4023-41c7-4510-93b7-bb12217ed56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Text language  word_count  sentence_count  \\\n",
      "0      Hello, how are you?       en           6               1   \n",
      "1  Bonjour, comment ça va?       fr           6               1   \n",
      "2     नमस्ते, आप कैसे हैं?       hi           6               1   \n",
      "\n",
      "                        word_tokens            sentence_tokens  \n",
      "0      [Hello, ,, how, are, you, ?]      [Hello, how are you?]  \n",
      "1  [Bonjour, ,, comment, ça, va, ?]  [Bonjour, comment ça va?]  \n",
      "2     [नमस्ते, ,, आप, कैसे, हैं, ?]     [नमस्ते, आप कैसे हैं?]  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from langdetect import detect\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from langdetect import DetectorFactory\n",
    "\n",
    "# Ensure consistent results with langdetect\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "# Sample Data (as if read from CSV)\n",
    "data = {\n",
    "    'Text': [\n",
    "        \"Hello, how are you?\",\n",
    "        \"Bonjour, comment ça va?\",\n",
    "        \"नमस्ते, आप कैसे हैं?\",\n",
    "\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Convert the sample data into a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Language detection function\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return \"unknown\"  # In case the language cannot be detected\n",
    "\n",
    "# Word count function (tokenizes text into words)\n",
    "def word_count(text):\n",
    "    words = word_tokenize(text)\n",
    "    return len(words)\n",
    "\n",
    "# Sentence count function (tokenizes text into sentences)\n",
    "def sentence_count(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    return len(sentences)\n",
    "\n",
    "# Apply the functions to the dataframe\n",
    "df['language'] = df['Text'].apply(detect_language)\n",
    "df['word_count'] = df['Text'].apply(word_count)\n",
    "df['sentence_count'] = df['Text'].apply(sentence_count)\n",
    "df['word_tokens'] = df['Text'].apply(lambda x: word_tokenize(x))\n",
    "df['sentence_tokens'] = df['Text'].apply(lambda x: sent_tokenize(x))\n",
    "\n",
    "# Display the processed DataFrame (optional)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59478e70-d241-47bb-8407-83006a2ba6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\felin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\felin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a German sentence:  Ich laufe gerne im Park.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original and Stemmed Words:\n",
      "Original: Ich, Stemmed: ich\n",
      "Original: laufe, Stemmed: lauf\n",
      "Original: gerne, Stemmed: gern\n",
      "Original: im, Stemmed: im\n",
      "Original: Park, Stemmed: park\n",
      "Original: ., Stemmed: .\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab') # Download the punkt_tab resource\n",
    "\n",
    "# Create an instance of the SnowballStemmer for German\n",
    "stemmer = SnowballStemmer(\"german\")\n",
    "\n",
    "# Accept a sentence input from the user\n",
    "sentence = input(\"Enter a German sentence: \")\n",
    "\n",
    "# Tokenize the sentence into words\n",
    "words = word_tokenize(sentence)\n",
    "\n",
    "# Perform stemming for each word in the list\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "# Output the original and stemmed words\n",
    "print(\"\\nOriginal and Stemmed Words:\")\n",
    "for original, stemmed in zip(words, stemmed_words):\n",
    "    print(f\"Original: {original}, Stemmed: {stemmed}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9146aa41-17b7-4580-8532-86990e177c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\felin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\felin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\felin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter a sentence:  \"The quick brown fox jumps over the lazy dog.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('``', '``'), ('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.'), (\"''\", \"''\")]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('punkt_tab')  # This may not be necessary, but keeping it for completeness\n",
    "\n",
    "# Accept sentence from the user\n",
    "sentence = input(\"Please enter a sentence: \")\n",
    "\n",
    "# Tokenize the sentence into words\n",
    "words = word_tokenize(sentence)\n",
    "\n",
    "# Perform POS tagging\n",
    "pos_tags = pos_tag(words)\n",
    "\n",
    "# Print the POS tags\n",
    "print(pos_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "badedfad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'language':\n",
      " [-5.3646474e-04  2.3810801e-04  5.1115868e-03  9.0194996e-03\n",
      " -9.3105817e-03 -7.1176412e-03  6.4591132e-03  8.9828968e-03\n",
      " -5.0157169e-03 -3.7675719e-03  7.3834234e-03 -1.5338082e-03\n",
      " -4.5329770e-03  6.5577170e-03 -4.8615970e-03 -1.8221879e-03\n",
      "  2.8838872e-03  9.8978775e-04 -8.2891919e-03 -9.4487956e-03\n",
      "  7.3142950e-03  5.0697206e-03  6.7561041e-03  7.6594559e-04\n",
      "  6.3553466e-03 -3.4018364e-03 -9.5542887e-04  5.7787406e-03\n",
      " -7.5193024e-03 -3.9333673e-03 -7.5123105e-03 -9.3613472e-04\n",
      "  9.5457397e-03 -7.3263012e-03 -2.3354592e-03 -1.9433126e-03\n",
      "  8.0798836e-03 -5.9388769e-03  4.6167683e-05 -4.7591492e-03\n",
      " -9.6119754e-03  5.0150724e-03 -8.7612551e-03 -4.3907915e-03\n",
      " -4.3744913e-05 -3.0390202e-04 -7.6636383e-03  9.6157677e-03\n",
      "  4.9826694e-03  9.2392573e-03 -8.1592631e-03  4.4972054e-03\n",
      " -4.1479236e-03  8.2272000e-04  8.5027330e-03 -4.4630114e-03\n",
      "  4.5193853e-03 -6.7860056e-03 -3.5441772e-03  9.4036534e-03\n",
      " -1.5820283e-03  3.2915483e-04 -4.1401330e-03 -7.6809111e-03\n",
      " -1.5112988e-03  2.4737134e-03 -8.7960326e-04  5.5425814e-03\n",
      " -2.7489183e-03  2.2621735e-03  5.4662931e-03  8.3530582e-03\n",
      " -1.4515533e-03 -9.2171840e-03  4.3751276e-03  5.7006377e-04\n",
      "  7.4459342e-03 -8.1073237e-04 -2.6433528e-03 -8.7568974e-03\n",
      " -8.6828845e-04  2.8285165e-03  5.4088747e-03  7.0643225e-03\n",
      " -5.7070865e-03  1.8606416e-03  6.0950904e-03 -4.8015565e-03\n",
      " -3.1117562e-03  6.7999489e-03  1.6381909e-03  1.8553811e-04\n",
      "  3.4774316e-03  2.1916839e-04  9.6230395e-03  5.0614346e-03\n",
      " -8.9229802e-03 -7.0386170e-03  9.0364297e-04  6.4001060e-03]\n",
      "\n",
      "Words similar to 'language':\n",
      "[('on', 0.21869485080242157), ('a', 0.21619486808776855), ('intelligence', 0.09344906359910965), (')', 0.09288787096738815), ('computers', 0.08414461463689804), ('of', 0.07975373417139053), ('and', 0.06576366722583771), ('field', 0.06288429349660873), ('focuses', 0.05463537201285362), ('is', 0.027043215930461884)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\felin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Option 2: Word Embeddings (Word2Vec)\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Sample paragraph\n",
    "text = \"\"\"\n",
    "Natural language processing (NLP) is a field of artificial intelligence. \n",
    "It focuses on the interaction between computers and humans through language.\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize into sentences, then into words\n",
    "sentences = sent_tokenize(text)\n",
    "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "\n",
    "# Train Word2Vec model\n",
    "model = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Access word vector\n",
    "word = 'language'\n",
    "if word in model.wv:\n",
    "    print(f\"Vector for '{word}':\\n\", model.wv[word])\n",
    "else:\n",
    "    print(f\"'{word}' not found in the vocabulary.\")\n",
    "\n",
    "# Find similar words\n",
    "print(\"\\nWords similar to 'language':\")\n",
    "print(model.wv.most_similar('language'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ee58061-85e7-4d4c-9e68-da8e1439d5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-Words Matrix:\n",
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n",
      "Vocabulary (Feature Names): ['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "documents = [\"This is the first document.\",\n",
    "\t\t\t\"This document is the second document.\",\n",
    "\t\t\t\"And this is the third one.\",\n",
    "\t\t\t\"Is this the first document?\"]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(documents)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"Bag-of-Words Matrix:\")\n",
    "print(X.toarray())\n",
    "print(\"Vocabulary (Feature Names):\", feature_names)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48dfc152-6284-44fc-b91d-73303460a586",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\felin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Lemmatizer Menu ---\n",
      "1. Lemmatize a word without POS tag\n",
      "2. Lemmatize a word with POS tag (adjective)\n",
      "3. Lemmatize a word with POS tag (verb)\n",
      "4. Exit\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your choice (1/2/3/4):  1\n",
      "Enter the word to lemmatize:  studies\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized word: study\n",
      "\n",
      "--- Lemmatizer Menu ---\n",
      "1. Lemmatize a word without POS tag\n",
      "2. Lemmatize a word with POS tag (adjective)\n",
      "3. Lemmatize a word with POS tag (verb)\n",
      "4. Exit\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your choice (1/2/3/4):  2\n",
      "Enter the word to lemmatize:  running\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized word (as adjective): running\n",
      "\n",
      "--- Lemmatizer Menu ---\n",
      "1. Lemmatize a word without POS tag\n",
      "2. Lemmatize a word with POS tag (adjective)\n",
      "3. Lemmatize a word with POS tag (verb)\n",
      "4. Exit\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your choice (1/2/3/4):  3\n",
      "Enter the word to lemmatize:  running\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized word (as verb): run\n",
      "\n",
      "--- Lemmatizer Menu ---\n",
      "1. Lemmatize a word without POS tag\n",
      "2. Lemmatize a word with POS tag (adjective)\n",
      "3. Lemmatize a word with POS tag (verb)\n",
      "4. Exit\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your choice (1/2/3/4):  4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting...\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the 'wordnet' resource\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# import these modules\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_word(word, pos=None):\n",
    "    if pos:\n",
    "        return lemmatizer.lemmatize(word, pos=pos)\n",
    "    else:\n",
    "        return lemmatizer.lemmatize(word)\n",
    "\n",
    "def main():\n",
    "    while True:\n",
    "        # Display the menu\n",
    "        print(\"\\n--- Lemmatizer Menu ---\")\n",
    "        print(\"1. Lemmatize a word without POS tag\")\n",
    "        print(\"2. Lemmatize a word with POS tag (adjective)\")\n",
    "        print(\"3. Lemmatize a word with POS tag (verb)\")\n",
    "        print(\"4. Exit\")\n",
    "\n",
    "        choice = input(\"Enter your choice (1/2/3/4): \")\n",
    "\n",
    "        if choice == '1':\n",
    "            word = input(\"Enter the word to lemmatize: \")\n",
    "            print(f\"Lemmatized word: {lemmatize_word(word)}\")\n",
    "\n",
    "        elif choice == '2':\n",
    "            word = input(\"Enter the word to lemmatize: \")\n",
    "            print(f\"Lemmatized word (as adjective): {lemmatize_word(word, pos='a')}\")\n",
    "\n",
    "        elif choice == '3':\n",
    "            word = input(\"Enter the word to lemmatize: \")\n",
    "            print(f\"Lemmatized word (as verb): {lemmatize_word(word, pos='v')}\")\n",
    "\n",
    "        elif choice == '4':\n",
    "            print(\"Exiting...\")\n",
    "            break\n",
    "\n",
    "        else:\n",
    "            print(\"Invalid choice, please try again.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7265c47a-4dda-44a7-b1e8-6c62a127b92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\felin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\felin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\felin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Welcome to Sentiment and Emotion Analyzer\n",
      "1. Analyze Sentiment\n",
      "2. Analyze Emotion\n",
      "3. Analyze Both Sentiment and Emotion\n",
      "4. Exit\n",
      "\n",
      "Enter your choice (1/2/3/4): 1\n",
      "\n",
      "Enter text to analyze sentiment: I am happy to visit you .\n",
      "Sentiment: POSITIVE\n",
      "\n",
      "Enter your choice (1/2/3/4): 4\n",
      "Exiting the program. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "# Download required resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('vader_lexicon')  # Download the VADER lexicon\n",
    "nltk.download('punkt_tab')  # Download the punkt_tab data for tokenization\n",
    "\n",
    "# Initialize SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Basic emotion dictionary\n",
    "emotion_dict = {\n",
    "    'anger': ['angry', 'mad', 'rage'],\n",
    "    'joy': ['happy', 'joyful', 'excited'],\n",
    "    'sadness': ['sad', 'unhappy', 'depressed'],\n",
    "    'fear': ['scared', 'afraid', 'terrified'],\n",
    "    'surprise': ['surprised', 'shocked', 'amazed'],\n",
    "    'disgust': ['disgusted', 'gross']\n",
    "}\n",
    "\n",
    "# Sentiment Analysis using VADER\n",
    "def analyze_sentiment(text):\n",
    "    sentiment_scores = sia.polarity_scores(text)\n",
    "    return 'POSITIVE' if sentiment_scores['compound'] >= 0.05 else ('NEGATIVE' if sentiment_scores['compound'] <= -0.05 else 'NEUTRAL')\n",
    "\n",
    "# Emotion Detection based on keywords\n",
    "def analyze_emotion(text):\n",
    "    words = word_tokenize(text.lower())\n",
    "    word_counts = Counter(words)\n",
    "    emotion_scores = {emotion: sum(word_counts[key] for key in keywords) for emotion, keywords in emotion_dict.items()}\n",
    "    return max(emotion_scores, key=emotion_scores.get) if max(emotion_scores.values()) > 0 else 'Neutral'\n",
    "\n",
    "# Function to analyze both sentiment and emotion\n",
    "def analyze_text(text):\n",
    "    sentiment = analyze_sentiment(text)\n",
    "    emotion = analyze_emotion(text)\n",
    "    return sentiment, emotion\n",
    "\n",
    "# Menu-driven program\n",
    "def menu():\n",
    "    print(\"\\nWelcome to Sentiment and Emotion Analyzer\")\n",
    "    print(\"1. Analyze Sentiment\")\n",
    "    print(\"2. Analyze Emotion\")\n",
    "    print(\"3. Analyze Both Sentiment and Emotion\")\n",
    "    print(\"4. Exit\")\n",
    "\n",
    "    while True:\n",
    "        choice = input(\"\\nEnter your choice (1/2/3/4): \")\n",
    "\n",
    "        if choice == '1':\n",
    "            text = input(\"\\nEnter text to analyze sentiment: \")\n",
    "            sentiment = analyze_sentiment(text)\n",
    "            print(f\"Sentiment: {sentiment}\")\n",
    "\n",
    "        elif choice == '2':\n",
    "            text = input(\"\\nEnter text to analyze emotion: \")\n",
    "            emotion = analyze_emotion(text)\n",
    "            print(f\"Emotion: {emotion}\")\n",
    "\n",
    "        elif choice == '3':\n",
    "            text = input(\"\\nEnter text to analyze both sentiment and emotion: \")\n",
    "            sentiment, emotion = analyze_text(text)\n",
    "            print(f\"Sentiment: {sentiment}, Emotion: {emotion}\")\n",
    "\n",
    "        elif choice == '4':\n",
    "            print(\"Exiting the program. Goodbye!\")\n",
    "            break\n",
    "\n",
    "        else:\n",
    "            print(\"Invalid choice. Please select a valid option.\")\n",
    "\n",
    "# Call the menu function to start the program\n",
    "if __name__ == \"__main__\":\n",
    "    menu()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebea501",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
